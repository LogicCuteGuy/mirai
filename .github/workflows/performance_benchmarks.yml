name: Performance Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly benchmarks
    - cron: '0 2 * * *'

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  performance-benchmarks:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for regression detection

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Cache Cargo dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target/
        key: ${{ runner.os }}-cargo-benchmark-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-benchmark-
          ${{ runner.os }}-cargo-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential pkg-config libssl-dev

    - name: Build benchmarks
      run: |
        cd mirai
        cargo build --release --tests --features benchmarks

    - name: Download baseline benchmarks
      continue-on-error: true
      run: |
        cd mirai
        # Try to download baseline from previous successful run
        curl -f -o baseline_benchmarks.json \
          "https://api.github.com/repos/${{ github.repository }}/actions/artifacts/baseline-benchmarks/download" \
          -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" || echo "No baseline found"

    - name: Run performance benchmarks
      run: |
        cd mirai
        export GIT_COMMIT=${{ github.sha }}
        export UPDATE_BASELINE=${{ github.ref == 'refs/heads/main' && 'true' || 'false' }}
        cargo test --release benchmark_suite_runner -- --nocapture

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          mirai/ci_benchmark_results/
          mirai/baseline_benchmarks.json
        retention-days: 30

    - name: Upload baseline (main branch only)
      uses: actions/upload-artifact@v3
      if: github.ref == 'refs/heads/main' && success()
      with:
        name: baseline-benchmarks
        path: mirai/baseline_benchmarks.json
        retention-days: 90

    - name: Comment PR with results
      uses: actions/github-script@v6
      if: github.event_name == 'pull_request'
      with:
        script: |
          const fs = require('fs');
          const path = 'mirai/ci_benchmark_results/comprehensive_report.md';
          
          if (fs.existsSync(path)) {
            const report = fs.readFileSync(path, 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸ“Š Performance Benchmark Results\n\n${report}`
            });
          }

    - name: Check for performance regressions
      run: |
        cd mirai
        if [ -f "ci_benchmark_results/regression_report.md" ]; then
          echo "## Performance Regression Report" >> $GITHUB_STEP_SUMMARY
          cat ci_benchmark_results/regression_report.md >> $GITHUB_STEP_SUMMARY
        fi
        
        # Fail if regressions detected
        if grep -q "Performance Assessment: FAIL" ci_benchmark_results/comprehensive_report.md; then
          echo "âŒ Performance regressions detected!"
          exit 1
        fi

  memory-leak-detection:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Install Valgrind
      run: |
        sudo apt-get update
        sudo apt-get install -y valgrind

    - name: Build with debug info
      run: |
        cd mirai
        cargo build --tests --features memory-testing

    - name: Run memory leak detection
      run: |
        cd mirai
        valgrind --tool=memcheck --leak-check=full --show-leak-kinds=all \
          --track-origins=yes --verbose --log-file=valgrind.log \
          cargo test memory_leak_detection -- --nocapture || true

    - name: Analyze memory results
      run: |
        cd mirai
        if grep -q "definitely lost" valgrind.log; then
          echo "âŒ Memory leaks detected!"
          cat valgrind.log
          exit 1
        else
          echo "âœ… No memory leaks detected"
        fi

    - name: Upload memory analysis
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: memory-analysis-${{ github.sha }}
        path: mirai/valgrind.log

  load-testing:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install aiohttp psutil

    - name: Build server
      run: |
        cd mirai
        cargo build --release --bin mirai-server

    - name: Start test server
      run: |
        cd mirai
        cargo run --release --bin mirai-server -- --config test_configs/load_test.toml &
        echo $! > server.pid
        sleep 10  # Wait for server to start

    - name: Run load tests
      run: |
        cd mirai
        python scripts/load_test.py --connections 100 --duration 60 --output load_test_results.json

    - name: Stop test server
      run: |
        cd mirai
        if [ -f server.pid ]; then
          kill $(cat server.pid) || true
        fi

    - name: Analyze load test results
      run: |
        cd mirai
        python scripts/compare_benchmarks.py load_test_results.json

    - name: Upload load test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: load-test-results-${{ github.sha }}
        path: |
          mirai/load_test_results.json
          mirai/load_test_report.md

  benchmark-comparison:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks]
    if: github.event_name == 'pull_request'

    steps:
    - uses: actions/checkout@v4

    - name: Download current benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results-${{ github.sha }}
        path: current-results/

    - name: Download baseline benchmark results
      uses: actions/download-artifact@v3
      continue-on-error: true
      with:
        name: baseline-benchmarks
        path: baseline-results/

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install matplotlib seaborn pandas

    - name: Generate benchmark comparison charts
      run: |
        python mirai/scripts/generate_benchmark_charts.py \
          --current current-results/all_benchmarks.json \
          --baseline baseline-results/baseline_benchmarks.json \
          --output benchmark_charts/

    - name: Upload benchmark charts
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-charts-${{ github.sha }}
        path: benchmark_charts/

  nightly-report:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks, memory-leak-detection, load-testing]
    if: github.event_name == 'schedule'

    steps:
    - uses: actions/checkout@v4

    - name: Download all results
      uses: actions/download-artifact@v3

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install jinja2 matplotlib seaborn pandas

    - name: Generate nightly report
      run: |
        python mirai/scripts/generate_nightly_report.py \
          --benchmark-results benchmark-results-*/all_benchmarks.json \
          --memory-analysis memory-analysis-*/valgrind.log \
          --load-test-results load-test-results-*/load_test_results.json \
          --output nightly_report.html

    - name: Upload nightly report
      uses: actions/upload-artifact@v3
      with:
        name: nightly-report-${{ github.run_number }}
        path: nightly_report.html

    # Optional: Send report to team (requires setup)
    # - name: Send report notification
    #   run: |
    #     # Send email, Slack notification, etc.
    #     echo "Nightly performance report generated"